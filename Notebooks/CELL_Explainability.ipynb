{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6jE59uwasYU"
      },
      "source": [
        "# CELL your Model: Contrastive Explanations for Large Language Models\n",
        "\n",
        "The advent of black-box deep neural network classification models has sparked the need to explain their decisions.\n",
        "However, in the case of generative AI, such as large language models (LLMs), there is no class prediction to explain.\n",
        "Rather, one can ask why an LLM output a particular response to a given prompt. In this paper, we answer this\n",
        "question by proposing, to the best of our knowledge, the first contrastive explanation methods requiring simply black-\n",
        "box/query access. Our explanations suggest that an LLM outputs a reply to a given prompt because if the prompt\n",
        "was slightly modified, the LLM would have given a different response that is either less preferable or contradicts the\n",
        "original response. The key insight is that contrastive explanations simply require a scoring function that has meaning\n",
        "to the user and not necessarily a specific real valued quantity (viz. class label). We offer two algorithms for finding\n",
        "contrastive explanations: i) A myopic algorithm, which although effective in creating contrasts, requires many model\n",
        "calls and ii) A budgeted algorithm, our main algorithmic contribution, which intelligently creates contrasts adhering\n",
        "to a query budget, necessary for longer contexts. We show the efficacy of these methods on diverse natural language\n",
        "tasks such as open-text generation, automated red teaming, and explaining conversational degradation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHAJieiZZGa-",
        "outputId": "4bef360c-d79a-474f-a121-070cac869d17"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "from cell import cell_algorithm\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "# 1) Set your API key\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # or hard-code temporarily, e.g. \"sk-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A sample use is shown as below. . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A sample prompt (feel free to adjust)\n",
        "original_prompt = (\n",
        "    \"To avoid financial ruin, wealthy individuals should prioritize living below their means, \"\n",
        "    \"diversify their investments, and cultivate a long-term perspective on wealth.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = cell_algorithm(original_prompt, client, split_k=1, delta=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if result:\n",
        "    print(\"\\nFinal Contrastive Explanation:\")\n",
        "    print(\"Original Prompt:\", result[\"original_prompt\"])\n",
        "    print(\"Original Response:\", result[\"original_response\"])\n",
        "    print(\"Contrastive Prompt:\", result[\"contrastive_prompt\"])\n",
        "    print(\"Contrastive Response:\", result[\"contrastive_response\"])\n",
        "    print(\"Contrast Score:\", result[\"contrast_score\"])\n",
        "\n",
        "    # Show iteration data in a pandas DataFrame\n",
        "    if \"iterations\" in result:\n",
        "        df = pd.DataFrame(result[\"iterations\"])\n",
        "        display(df)\n",
        "\n",
        "    # Side-by-side prompt diff\n",
        "    prompt_diff_html = generate_diff_html(\n",
        "        result[\"original_prompt\"],\n",
        "        result[\"contrastive_prompt\"]\n",
        "    )\n",
        "    display(HTML(prompt_diff_html))\n",
        "\n",
        "    # Side-by-side response diff\n",
        "    response_diff_html = generate_diff_html(\n",
        "        result[\"original_response\"],\n",
        "        result[\"contrastive_response\"]\n",
        "    )\n",
        "    display(HTML(response_diff_html))\n",
        "else:\n",
        "    print(\"Failed to find a satisfactory contrastive explanation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "IcBPukyCjsif",
        "outputId": "015f75a7-3b4e-491c-fc3a-1079bab885c7"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us plot heatmap . . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from matplotlib import cm, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "cz2TS33pjPdt",
        "outputId": "6ef40610-3cc1-4a81-9b80-6ad58df4dbd3"
      },
      "outputs": [],
      "source": [
        "words = original_prompt.split(' ')\n",
        "save_path = 'output/text_heatmap_1.png'\n",
        "\n",
        "print(len(words))\n",
        "\n",
        "coefficients = df['score'].iloc[1:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the function with the save path\n",
        "plot_text_heatmap(words, coefficients, title=\"Text Heatmap 1\", save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "c5Zgy8KomCzi",
        "outputId": "09bf86f4-359c-4b14-93e7-99f87fe3cdaf"
      },
      "outputs": [],
      "source": [
        "save_path = 'output/text_heatmap_2.png'\n",
        "\n",
        "print(len(words))\n",
        "\n",
        "coefficients = df['score'].iloc[1:].values\n",
        "\n",
        "# Call the function with the save path\n",
        "plot_text_heatmap(words, coefficients, title=\"Text Heatmap 2\", save_path=save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cell",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
